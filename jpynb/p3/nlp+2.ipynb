{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUnLBypJRAyL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTRwquUiRzrt"
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(open('training_data.txt', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_UNK(words):\n",
    "  count = {}\n",
    "  for word in words:\n",
    "    try:\n",
    "      count[word] += 1\n",
    "    except:\n",
    "      count[word] = 1\n",
    "  for i in range(len(words)):\n",
    "    if count[words[i]]<=3:\n",
    "      words[i] = 'UNK'\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfzDnm7cR6Ln"
   },
   "outputs": [],
   "source": [
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "def train(tokens):\n",
    "    tokens = set_UNK(tokens)\n",
    "    unigram = {}\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            unigram[token] += 1\n",
    "        except:\n",
    "            unigram[token] = 1\n",
    "\n",
    "    bigram = {}\n",
    "    for i in range(1,len(tokens)):\n",
    "        try:\n",
    "            bigram[(tokens[i-1],tokens[i])] += 1\n",
    "        except:\n",
    "            bigram[(tokens[i-1],tokens[i])] = 1\n",
    "\n",
    "    trigram = {}\n",
    "    for i in range(2,len(tokens)):\n",
    "        try:\n",
    "            trigram[(tokens[i-2],tokens[i-1],tokens[i])] += 1\n",
    "        except:\n",
    "            trigram[(tokens[i-2],tokens[i-1],tokens[i])] = 1\n",
    "    return unigram, bigram, trigram\n",
    "unigram, bigram, trigram = train(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "import json\n",
    "with open('unigram.txt','w') as f:\n",
    "    f.write(str(unigram))\n",
    "with open('trigram.txt','w') as f:\n",
    "    f.write(str(trigram))\n",
    "with open('bigram.txt','w') as f:\n",
    "    f.write(str(bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AnF5L-81RYn"
   },
   "outputs": [],
   "source": [
    "def uni(key):\n",
    "    if key in unigram:\n",
    "        return unigram[key]\n",
    "    else:\n",
    "        return unigram['UNK']\n",
    "\n",
    "def bi(key):\n",
    "    if key in bigram:\n",
    "        return bigram[key]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tri(key):\n",
    "    if key in trigram:\n",
    "        return trigram[key]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOUM1SX1p8Yb"
   },
   "source": [
    "\n",
    "$$\n",
    "P(w_n)=\\frac{N(w_n)}{N(words)}\\\\\n",
    "P(w_n|w_{n-1})=\\frac{N(w_{n-1},w_n)}{N(w_{n-1})}\\\\\n",
    "P(w_n|w_{n-2},w_{n-1})=\\frac{N(w_{n-2},w_{n-1},w_n)}{N(w_{n-2},w_{n-1})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vGSE3W5vPKY"
   },
   "outputs": [],
   "source": [
    "def cal_pp_linear(words, lamda):\n",
    "    N = len(words)\n",
    "    n_uni = len(words)\n",
    "    log_P = 0\n",
    "    for i in range(2, N):\n",
    "        P_uni = uni(words[i])/n_uni\n",
    "        P_bi = bi((words[i-1],words[i]))/uni(words[i-1])\n",
    "        try:\n",
    "            P_tri = tri((words[i-2],words[i-1],words[i]))/bi((words[i-2],words[i-1]))\n",
    "        except:\n",
    "            P_tri = 0\n",
    "        P_i = P_uni*lamda[0] + P_bi*lamda[1] + P_tri*lamda[2]\n",
    "        \n",
    "        try:\n",
    "            log_P += math.log(P_i)\n",
    "        except:\n",
    "            print(words, str(lamda))\n",
    "    return math.exp(-log_P/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: [0.8, 0.1, 0.1] perplexity: 85.699660\n",
      "lambda: [0.6, 0.2, 0.2] perplexity: 76.248407\n",
      "lambda: [0.4, 0.3, 0.3] perplexity: 74.142995\n",
      "lambda: [0.1, 0.8, 0.1] perplexity: 89.932246\n",
      "lambda: [0.2, 0.6, 0.2] perplexity: 79.219987\n",
      "lambda: [0.3, 0.4, 0.3] perplexity: 75.163013\n",
      "lambda: [0.1, 0.1, 0.8] perplexity: 107.639573\n",
      "lambda: [0.2, 0.2, 0.6] perplexity: 83.932997\n",
      "lambda: [0.3, 0.3, 0.4] perplexity: 75.808752\n"
     ]
    }
   ],
   "source": [
    "held_out = tokens[:math.floor(0.2*len(tokens))]\n",
    "unigram, bigram, trigram = train(tokens[math.floor(0.2*len(tokens)):])\n",
    "##\n",
    "\n",
    "lamdas = [[0.8, 0.1, 0.1],\n",
    "      [0.6, 0.2, 0.2],\n",
    "      [0.4, 0.3, 0.3],\n",
    "      [0.1, 0.8, 0.1],\n",
    "      [0.2, 0.6, 0.2],\n",
    "      [0.3, 0.4, 0.3],\n",
    "      [0.1, 0.1, 0.8],\n",
    "      [0.2, 0.2, 0.6],\n",
    "      [0.3, 0.3, 0.4],\n",
    "      ]\n",
    "\n",
    "# find lambda\n",
    "def find_lambda(words, lamdas):\n",
    "    min_perplexity = float('inf')\n",
    "    min_lamda = []\n",
    "    for lamda in lamdas:\n",
    "        perplexity = cal_pp_linear(words, lamda)\n",
    "        print('lambda: %s perplexity: %f'%(str(lamda), perplexity))\n",
    "        if perplexity < min_perplexity:\n",
    "            min_perplexity = perplexity\n",
    "            min_lamda = lamda\n",
    "    return min_lamda\n",
    "\n",
    "best_lambda = find_lambda(held_out, lamdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = nltk.word_tokenize(open('testing_data.txt', encoding='utf-8').read())\n",
    "test_words = set_UNK(test_words)\n",
    "\n",
    "unigram, bigram, trigram = train(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.55379562841097"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_pp_linear(test_words,best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-S6MDYB-FDJ"
   },
   "outputs": [],
   "source": [
    "#bigram\n",
    "def cal_pp_add(words, lamda):\n",
    "  N = len(words)\n",
    "  V = len(words)\n",
    "  log_P = 0\n",
    "  for i in range(1, V):\n",
    "    P = (bi((words[i-1],words[i])) + lamda)/(uni(words[i-1]) + lamda*V)\n",
    "    log_P += math.log(P)\n",
    "  return math.exp(-log_P/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "A3BSQXjF8H5B",
    "outputId": "3aadbc2b-24c3-4eec-8fd0-799b9da4db43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1533.108936679848\n",
      "2512.675687167015\n",
      "3417.2315494894638\n"
     ]
    }
   ],
   "source": [
    "print(cal_pp_add(test_words, 0.1))\n",
    "print(cal_pp_add(test_words, 0.2))\n",
    "print(cal_pp_add(test_words, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
